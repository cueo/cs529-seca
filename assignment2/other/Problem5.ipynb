{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "638939ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff83297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the logistic regression (multi class; implemented from scratch) training algorithm and plot the training \n",
    "# and test errors.\n",
    "\n",
    "tra_df = pd.read_csv('optdigits.tra', header=None, sep=',')\n",
    "tes_df = pd.read_csv('optdigits.tes', header=None, sep=',')\n",
    "tra_lbl = tra_df.pop(64)\n",
    "tes_lbl = tes_df.pop(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36f855d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardScaler(feature_array):\n",
    "    total_cols = feature_array.shape[1] # total number of columns \n",
    "    for i in range(total_cols): # iterating through each column\n",
    "        feature_col = feature_array[i]\n",
    "        mean = feature_col.mean() # mean stores mean value for the column\n",
    "        std = feature_col.std() # std stores standard deviation value for the column\n",
    "        if(std==float(0)):\n",
    "            std=1\n",
    "        feature_array[i] = (feature_array[i] - mean) / std\n",
    "        \n",
    "def sigmoid(z):\n",
    "    return 1.0/(1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    loss = -np.mean(y*(np.log(y_hat)) - (1-y)*np.log(1-y_hat))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def gradients(X, y, y_hat,w=None,Lambda=None):\n",
    "    \n",
    "    # X --> Input.\n",
    "    # y --> true/target value.\n",
    "    # y_hat --> hypothesis/predictions.\n",
    "    # w --> weights (parameter).\n",
    "    # b --> bias (parameter).\n",
    "    \n",
    "    # m-> number of training examples.\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Gradient of loss w.r.t weights.\n",
    "    if(Lambda==None):\n",
    "        dw = (1/m)*np.dot(X.T, (y_hat - y))\n",
    "    else:\n",
    "        dw = (1/m)*(np.dot(X.T, (y_hat - y)) + Lambda*w)\n",
    "    \n",
    "    # Gradient of loss w.r.t bias.\n",
    "    db = (1/m)*np.sum((y_hat - y)) \n",
    "    \n",
    "    return dw, db\n",
    "\n",
    "def train(X, y, bs, epochs, lr,Lambda=None):\n",
    "    \n",
    "    # X --> Input.\n",
    "    # y --> true/target value.\n",
    "    # bs --> Batch Size.\n",
    "    # epochs --> Number of iterations.\n",
    "    # lr --> Learning rate.\n",
    "        \n",
    "    # m-> number of training examples\n",
    "    # n-> number of features \n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initializing weights and bias to zeros.\n",
    "    w = np.zeros((n,1))\n",
    "    b = 0\n",
    "    \n",
    "    # Reshaping y.\n",
    "    y = y.reshape(m,1)\n",
    "    \n",
    "    # Normalizing the inputs.\n",
    "    \n",
    "    # Empty list to store losses.\n",
    "    losses = []\n",
    "    \n",
    "    # Training loop.\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((m-1)//bs + 1):\n",
    "            \n",
    "            # Defining batches. SGD.\n",
    "            start_i = i*bs\n",
    "            end_i = start_i + bs\n",
    "            xb = X[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            \n",
    "            # Calculating hypothesis/prediction.\n",
    "            y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "            \n",
    "            # Getting the gradients of loss w.r.t parameters.\n",
    "            dw, db = gradients(xb, yb, y_hat,w,Lambda)\n",
    "            \n",
    "            # Updating the parameters.\n",
    "            w -= lr*dw\n",
    "            b -= lr*db\n",
    "        \n",
    "        # Calculating loss and appending it in the list.\n",
    "        l = loss(y, sigmoid(np.dot(X, w) + b))\n",
    "        losses.append(l)\n",
    "        \n",
    "    # returning weights, bias and losses(List).\n",
    "    return w, b, losses\n",
    "\n",
    "def predict(X,w,b):\n",
    "    \n",
    "    # Calculating presictions/y_hat.\n",
    "    preds = sigmoid(np.dot(X, w) + b)\n",
    "    \n",
    "    # Empty List to store predictions.\n",
    "    pred_class = []\n",
    "    # if y_hat >= 0.5 --> round up to 1\n",
    "    # if y_hat < 0.5 --> round up to 1\n",
    "    pred_class = [1 if i > 0.5 else 0 for i in preds]\n",
    "    return np.array(pred_class),np.array(preds.reshape(1,-1)[0])\n",
    "\n",
    "def convert_labels(train_label,test_label):\n",
    "    n_class = np.unique(train_label)\n",
    "    train_labels,test_labels = [],[]\n",
    "    for i in n_class:\n",
    "        train_labels.append(np.where(train_label == i, 1, 0))\n",
    "        test_labels.append(np.where(test_label == i, 1, 0))\n",
    "    return train_labels,test_labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f8afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the Training and test Dataset\n",
    "standardScaler(tra_df)\n",
    "standardScaler(tes_df)\n",
    "\n",
    "# Building label data sets for each class to build 0-1 Logistic Regression models.\n",
    "train_labels,test_labels = convert_labels(tra_lbl,tes_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f27f91f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training \n",
    "model_w,model_b,model_l = [],[],[]\n",
    "for i in np.unique(tra_lbl):\n",
    "    w, b, l = train(tra_df, train_labels[i], bs=100, epochs=1000, lr=0.01)\n",
    "    model_w.append(w)\n",
    "    model_b.append(b)\n",
    "    model_l.append(l)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a09609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "train_preds,test_preds = pd.DataFrame(),pd.DataFrame()\n",
    "for i in np.unique(tra_lbl):\n",
    "    _,train_preds[i] = predict(tra_df,model_w[i],model_b[i])\n",
    "    _,test_preds[i] = predict(tes_df,model_w[i],model_b[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af82e16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  97.12267852471881\n",
      "Test Accuracy:  94.54646633277684\n"
     ]
    }
   ],
   "source": [
    "#Train error and Test error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_accuracy=accuracy_score(tra_lbl, train_preds.idxmax(axis=1))*100\n",
    "test_accuracy=accuracy_score(tes_lbl, test_preds.idxmax(axis=1))*100\n",
    "print(\"Train Accuracy: \",train_accuracy)\n",
    "print(\"Test Accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d05cdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akarshbolar/opt/anaconda3/envs/cs529/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n",
      "/Users/akarshbolar/opt/anaconda3/envs/cs529/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in multiply\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  98.5874967303165\n",
      "Test Accuracy:  94.65776293823038\n",
      "------------------------------ 0.0001\n",
      "Train Accuracy:  98.5874967303165\n",
      "Test Accuracy:  94.65776293823038\n",
      "------------------------------ 0.001\n",
      "Train Accuracy:  98.29976458278838\n",
      "Test Accuracy:  94.88035614913744\n",
      "------------------------------ 0.1\n"
     ]
    }
   ],
   "source": [
    "# Use the logistic regression classifier with regularization so that you also penalize large weights (λ∥w∥2). \n",
    "# Plot the average training and test errors for at least 5 different values of regularization parameter λ.\n",
    "\n",
    "reg_values = [0.0001,0.001,0.1]\n",
    "\n",
    "for val in reg_values:\n",
    "    model_w,model_b,model_l = [],[],[]\n",
    "    for i in np.unique(tra_lbl):\n",
    "        w, b, l = train(tra_df, train_labels[i], bs=500, epochs=5000, lr=0.5,Lambda=val)\n",
    "        model_w.append(w)\n",
    "        model_b.append(b)\n",
    "        model_l.append(l)\n",
    "\n",
    "    train_preds,test_preds = pd.DataFrame(),pd.DataFrame()\n",
    "    for i in np.unique(tra_lbl):\n",
    "        _,train_preds[i] = predict(tra_df,model_w[i],model_b[i])\n",
    "        _,test_preds[i] = predict(tes_df,model_w[i],model_b[i])\n",
    "\n",
    "    train_accuracy=accuracy_score(tra_lbl, train_preds.idxmax(axis=1))*100\n",
    "    test_accuracy=accuracy_score(tes_lbl, test_preds.idxmax(axis=1))*100\n",
    "    print(\"Train Accuracy: \",train_accuracy)\n",
    "    print(\"Test Accuracy: \", test_accuracy)\n",
    "    print(\"------------------------------\",val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a25a5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-CS529",
   "language": "python",
   "name": "cs529"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
